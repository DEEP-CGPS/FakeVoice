{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8098bd0c",
   "metadata": {},
   "source": [
    "\n",
    "# Create & Save a Custom CNN (TorchScript) for **fakevoicefinder**\n",
    "\n",
    "This notebook shows how to define a **custom CNN** that:\n",
    "- Works with **variable input sizes** (independent of spectrogram width/height) thanks to **AdaptiveAvgPool2d**.\n",
    "- Outputs **2 classes** (real vs fake).\n",
    "- Is exported as **TorchScript** (`.pt`) so the library can load it as a **user model** (`usermodel_jit`).\n",
    "\n",
    "> **Save location**: by default we save to `../models/SimpleCNN_scripted.pt` (assuming this notebook lives in a `notebooks/` folder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a69ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you need to install PyTorch, follow your platform instructions at https://pytorch.org/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8369dbe",
   "metadata": {},
   "source": [
    "\n",
    "## Define a size-agnostic CNN\n",
    "\n",
    "Key idea: use **`nn.AdaptiveAvgPool2d((1, 1))`** before the final classifier so the model\n",
    "does **not** depend on the input spatial size. This is perfect for spectrograms where time width varies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1944c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A small, size-agnostic CNN for 2-class classification.\n",
    "\n",
    "    Design goals\n",
    "    ------------\n",
    "    - Accepts inputs shaped [B, C, H, W] where C is typically 1 (mel/log spectrograms).\n",
    "    - Uses AdaptiveAvgPool2d((1,1)) so the spatial dimensions do not constrain the model.\n",
    "    - Final output has **2 logits** (real=0, fake=1). Apply Softmax externally if you need probabilities.\n",
    "\n",
    "    TorchScript-compatibility:\n",
    "    - The module is simple (pure nn layers) and can be exported with torch.jit.script.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int = 1, hidden_channels: int = 32):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),   # downsample by 2\n",
    "\n",
    "            nn.Conv2d(hidden_channels, hidden_channels * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),   # downsample by 2\n",
    "\n",
    "            nn.Conv2d(hidden_channels * 2, hidden_channels * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # No more maxpool to keep it lightweight; we rely on AdaptiveAvgPool below\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))     # size-agnostic pooling\n",
    "        self.classifier = nn.Linear(hidden_channels * 4, 2)  # 2 classes\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)        # [B, C*, H', W']\n",
    "        x = self.gap(x)             # [B, C*, 1, 1]\n",
    "        x = torch.flatten(x, 1)     # [B, C*]\n",
    "        logits = self.classifier(x) # [B, 2]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cef72d",
   "metadata": {},
   "source": [
    "\n",
    "## Sanity check with variable input sizes\n",
    "\n",
    "We test two spectrogram-like tensors with **different widths**; output must always be `[B, 2]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5dacc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes: torch.Size([1, 2]) torch.Size([1, 2])\n",
      "✅ Size-agnostic forward pass OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate\n",
    "model = SimpleCNN(in_channels=1, hidden_channels=32)\n",
    "model.eval()\n",
    "\n",
    "# Two inputs with different spatial sizes (H=128, W varies)\n",
    "x1 = torch.randn(1, 1, 128, 300)\n",
    "x2 = torch.randn(1, 1, 128, 552)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y1 = model(x1)\n",
    "    y2 = model(x2)\n",
    "\n",
    "print(\"Output shapes:\", y1.shape, y2.shape)  # both should be [1, 2]\n",
    "assert y1.shape == (1, 2) and y2.shape == (1, 2), \"Model output should be [B, 2] regardless of input size.\"\n",
    "print(\"✅ Size-agnostic forward pass OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c6891e",
   "metadata": {},
   "source": [
    "\n",
    "## Export to TorchScript and save under `../models`\n",
    "\n",
    "The library will look for user models in your **`models/`** folder. We save a `*.pt`\n",
    "TorchScript file there, which the loader will detect and copy into the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b648f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TorchScript saved to: D:\\UMNG-2025\\FakeVoice\\FakeVoice\\models\\SimpleCNN_scripted.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_dir = Path(\"../models\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_path = save_dir / \"SimpleCNN_scripted.pt\"\n",
    "\n",
    "# Script and save\n",
    "scripted = torch.jit.script(model)\n",
    "scripted.save(str(save_path))\n",
    "\n",
    "print(f\"✅ TorchScript saved to: {save_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100659c",
   "metadata": {},
   "source": [
    "\n",
    "## Reload test (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f57180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reloaded = torch.jit.load(str(save_path), map_location=\"cpu\")\n",
    "reloaded.eval()\n",
    "with torch.no_grad():\n",
    "    y = reloaded(torch.randn(2, 1, 128, 400))\n",
    "print(\"Reloaded output shape:\", y.shape)  # [2, 2]\n",
    "print(\"✅ Reload test OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e277d",
   "metadata": {},
   "source": [
    "\n",
    "## Use this model in **fakevoicefinder**\n",
    "\n",
    "Once saved in `models/`, your experiment can load it as a *user TorchScript* model:\n",
    "\n",
    "```python\n",
    "from fakevoicefinder.experiment import CreateExperiment\n",
    "from fakevoicefinder.model_loader import ModelLoader\n",
    "\n",
    "# (1) Build your experiment as usual (cfg already validated)\n",
    "exp = CreateExperiment(cfg, experiment_name=cfg.run_name)\n",
    "exp.build()\n",
    "\n",
    "# (2) Prepare benchmarks (optional)\n",
    "loader = ModelLoader(exp)\n",
    "loader.prepare_benchmarks(add_softmax=False, input_channels=1)  # or 3 if your transforms are RGB-like\n",
    "\n",
    "# (3) Prepare user models (TorchScript)\n",
    "#    This will detect ../models/SimpleCNN_scripted.pt and register it as 'usermodel_<file>'\n",
    "loader.prepare_user_models()\n",
    "\n",
    "# (4) Proceed to training with Trainer(exp) ...\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
